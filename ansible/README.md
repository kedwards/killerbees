# Ansible - Docker Swarm Infrastructure Automation

This directory contains Ansible playbooks and configurations for automating the deployment of a complete Docker Swarm infrastructure with GlusterFS, Traefik, HAProxy, and Portainer. The automation handles everything from initial system setup to service deployment and configuration.

## üèóÔ∏è Infrastructure Overview

The Ansible automation creates a robust Docker Swarm cluster featuring:

- **Docker Swarm Cluster**: 3-node manager setup with optional worker nodes
- **GlusterFS**: Distributed filesystem for persistent container volumes
- **Traefik**: Layer 7 reverse proxy with automatic service discovery
- **HAProxy**: Load balancer with SSL offloading
- **Portainer**: Web-based Docker management interface

## üìã Requirements

### Host System Requirements
- **Ansible**: CLI utilities installed on your control machine
- **SSH Access**: Passwordless SSH to target hosts
- **Python**: Available on all target hosts (usually pre-installed)

### Development Environment
- **[Vagrant](https://www.vagrantup.com/)**: For local development environments
- **[Vai Plugin](https://github.com/cjsteel/vagrant-plugin-vai)**: Vagrant plugin for Ansible inventory generation
- **[vagrant-hostmanager](https://github.com/devopsgroup-io/vagrant-hostmanager)**: Manages hosts file on guest machines
- **[vagrant-vbguest](https://github.com/dotless-de/vagrant-vbguest)**: Keeps VirtualBox Guest Additions updated

### Target Infrastructure Requirements
- **4 Hosts** running Ubuntu 18.04+ or compatible Debian-based system
- **Primary user** with sudo privileges
- **SSH service** enabled and accessible
- **Hardware per host**:
  - 1+ CPU cores
  - 1+ GB RAM
  - 20+ GB primary drive
  - 10+ GB secondary drive (for Swarm nodes with GlusterFS)

## üöÄ Installation & Setup

### Install Ansible

**For macOS with Homebrew:**
```bash
brew install ansible
```

**For Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install ansible
```

## üìÅ Project Structure

```
ansible/
‚îú‚îÄ‚îÄ inventory/              # Inventory files for different environments
‚îÇ   ‚îú‚îÄ‚îÄ hosts              # Production inventory
‚îÇ   ‚îú‚îÄ‚îÄ hosts-vagrant      # Vagrant inventory
‚îÇ   ‚îî‚îÄ‚îÄ vagrant_ansible_inventory  # Auto-generated by Vagrant
‚îú‚îÄ‚îÄ playbooks/             # Ansible playbooks
‚îÇ   ‚îú‚îÄ‚îÄ install.yml        # Main installation playbook
‚îÇ   ‚îú‚îÄ‚îÄ config.yml         # Configuration variables
‚îÇ   ‚îú‚îÄ‚îÄ upgrade-docker.yml # Docker upgrade playbook
‚îÇ   ‚îú‚îÄ‚îÄ upgrade-packages.yml # System package updates
‚îÇ   ‚îî‚îÄ‚îÄ redeploy-apps.yml  # Application redeployment
‚îú‚îÄ‚îÄ roles/                 # Ansible roles (if any)
‚îî‚îÄ‚îÄ README.md             # This file
```

## üîß Configuration

### Inventory Groups

The Ansible inventory defines several groups for different node types:

- **`[haproxy]`**: Single node configured as the front-end load balancer
- **`[swarm_managers]`**: Must define at minimum 3 nodes (for high availability)
- **`[swarm_workers]`**: Optional group for additional worker nodes
- **`[gluster_nodes]`**: Must define exactly 3 nodes for GlusterFS cluster

### Sample Inventory (Vagrant)

```ini
# Generated by Vagrant
worker01 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2203 ansible_ssh_private_key_file=/home/user/.vagrant.d/insecure_private_keys/vagrant.key.ed25519 ansible_ssh_user=vagrant
worker02 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2204 ansible_ssh_private_key_file=/home/user/.vagrant.d/insecure_private_keys/vagrant.key.ed25519 ansible_ssh_user=vagrant
manager03 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2202 ansible_ssh_private_key_file=/home/user/.vagrant.d/insecure_private_keys/vagrant.key.ed25519 ansible_ssh_user=vagrant
proxy01 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_private_key_file=/home/user/.vagrant.d/insecure_private_keys/vagrant.key.ed25519 ansible_ssh_user=vagrant
manager02 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2201 ansible_ssh_private_key_file=/home/user/.vagrant.d/insecure_private_keys/vagrant.key.ed25519 ansible_ssh_user=vagrant
manager01 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2200 ansible_ssh_private_key_file=/home/user/.vagrant.d/insecure_private_keys/vagrant.key.ed25519 ansible_ssh_user=vagrant

[proxy_nodes]
proxy01

[swarm_managers]
manager01
manager02
manager03

[gluster_nodes]
manager01
manager02
manager03

[swarm_workers]
worker01
worker02

[docker_nodes]
manager01
manager02
manager03
worker01
worker02

[docker:children]
proxy_nodes
docker_nodes
swarm_managers
swarm_workers
```

### Configuration Variables

Customization options are found in `playbooks/config.yml`:

- **Application settings**: DNS names, passwords, SSL configuration
- **Network configuration**: IP ranges, ports, certificates
- **Storage settings**: GlusterFS volume configuration
- **Service settings**: Traefik, Portainer, HAProxy configuration

## üöÄ Deployment

### 1. Test Connectivity

Verify Ansible can reach all hosts:

**For Vagrant Infrastructure:**
```bash
ansible all --inventory-file=hosts-vagrant -a "/bin/echo hello"
```

**For External Hosts:**
```bash
ansible all -a "/bin/echo hello"
```

### 2. Run the Main Playbook

**For Vagrant Infrastructure:**
```bash
ansible-playbook --inventory-file=hosts-vagrant playbooks/install.yml
```

**For External Hosts:**
```bash
ansible-playbook playbooks/install.yml
```

### 3. Post-Deployment Configuration

#### DNS Setup

For Layer 7 routing to work, configure DNS entries or local hosts file:

**Example entries for local development:**
```
10.10.10.10 portainer.docker.local
10.10.10.10 traefik.docker.local
10.10.10.10 wordpress.docker.local
```

## üéØ What the Playbook Does

The Ansible automation performs the following high-level actions:

1. **System Preparation**
   - Updates host packages and dependencies
   - Installs Docker CE on all swarm nodes
   - Configures user permissions for Docker access

2. **Docker Swarm Setup**
   - Initializes the first manager as the swarm leader
   - Joins remaining nodes as managers (minimum 3 for HA)
   - Joins worker nodes if defined

3. **Storage Configuration**
   - Configures secondary drives with XFS filesystem
   - Sets up GlusterFS cluster on designated nodes
   - Installs Docker GlusterFS storage plugin

4. **Service Deployment**
   - Installs Traefik stack as a global service
   - Deploys Portainer agents and UI
   - Sets up HAProxy with SSL certificates
   - Creates necessary Docker networks

## üåê Access Services

After deployment, access the management interfaces:

- **Traefik Dashboard**: http://traefik.docker.local (admin:password1234)
- **Portainer UI**: http://portainer.docker.local (admin:password1234)
- **HAProxy Stats**: http://10.10.10.10/stats (admin:password1234)

## üì¶ Deploying Applications

### Example: WordPress Stack

Create a `wordpress-stack.yml` file:

```yaml
version: '3.5'
services:
  wordpress:
    image: wordpress
    environment:
      WORDPRESS_DB_HOST: db
      WORDPRESS_DB_USER: exampleuser
      WORDPRESS_DB_PASSWORD: examplepass
      WORDPRESS_DB_NAME: exampledb
    ports:
      - "80"
    networks:
      - wordpress
      - web
    volumes:
      - wordpress_data:/var/www/html/wp-content
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.wordpress.rule=Host(`wordpress.docker.local`)"
        - "traefik.http.services.wordpress.loadbalancer.server.port=80"
        - "traefik.docker.network=web"

  db:
    image: mysql:5.7
    environment:
      MYSQL_DATABASE: exampledb
      MYSQL_USER: exampleuser
      MYSQL_PASSWORD: examplepass
      MYSQL_RANDOM_ROOT_PASSWORD: '1'
    networks:
      - wordpress
    volumes:
      - mysql_data:/var/lib/mysql

networks:
  wordpress:
    driver: overlay
    attachable: true
  web:
    external: true

volumes:
  wordpress_data:
    driver: glusterfs
    name: "gfs/wordpress_data"
  mysql_data:
    driver: glusterfs
    name: "gfs/wordpress_mysql"
```

Deploy the stack:

```bash
# Create storage directories
sudo mkdir /mnt/gfs/wordpress_data
sudo mkdir /mnt/gfs/wordpress_mysql

# Deploy the stack
docker stack deploy --compose-file=wordpress-stack.yml wordpress
```

## üîÑ Maintenance Operations

### Re-running the Playbook

The playbook is idempotent and can be re-run safely. Most tasks check for existing configuration and skip if already completed.

### Available Maintenance Playbooks

- **`upgrade-docker.yml`**: Updates Docker to the latest version
- **`upgrade-packages.yml`**: Updates system packages
- **`redeploy-apps.yml`**: Redeploys applications and services

### Example Usage

```bash
# Upgrade Docker on all nodes
ansible-playbook --inventory-file=hosts-vagrant playbooks/upgrade-docker.yml

# Update system packages
ansible-playbook --inventory-file=hosts-vagrant playbooks/upgrade-packages.yml
```

## üè≠ Production Considerations

### Security Enhancements

- Use SSH keys instead of passwords
- Implement proper firewall rules
- Use real SSL certificates (Let's Encrypt)
- Configure proper backup strategies

### Scaling Considerations

- Add more worker nodes by updating inventory
- Separate GlusterFS from Swarm managers
- Use external load balancer (AWS ALB, etc.)
- Implement monitoring and logging

### Example Production Inventory

```ini
[haproxy]
haproxy.example.com

[swarm_managers]
manager1.example.com
manager2.example.com
manager3.example.com

[swarm_workers]
worker1.example.com
worker2.example.com
worker3.example.com

[gluster_nodes]
gluster1.example.com
gluster2.example.com
gluster3.example.com

[all:vars]
ansible_user=ubuntu
ansible_ssh_private_key_file=/path/to/ssh_key.pem
```

## üîó Related Documentation

- [Main Project README](../README.md)
- [Vagrant Setup](../vagrant/README.md)
- [Packer Images](../packer/README.md)
- [Docker Swarm Documentation](https://docs.docker.com/engine/swarm/)
- [Traefik Documentation](https://doc.traefik.io/traefik/)
